# @package _global_
# Pick and place are kinematically simulated.
# Navigation is fully simulated.

defaults:
  - /benchmark/rearrange: rearrange_easy
  - /habitat_baselines: habitat_baselines_rl_config_base
  - /habitat_baselines/rl/policy/obs_transforms:
    - add_virtual_keys_base
  - /habitat/task/actions:
    - pddl_apply_action
    - oracle_nav_action
    - arm_action
    - base_velocity
    - rearrange_stop
  - _self_

habitat:
  gym:
    obs_keys:
      - robot_head_depth
      - relative_resting_position
      - obj_start_sensor
      - obj_goal_sensor
      - obj_start_gps_compass
      - obj_goal_gps_compass
      - joint
      - is_holding
      - localization_sensor

habitat_baselines:
  verbose: False
  trainer_name: "ddppo"
  torch_gpu_id: 0
  rollout_storage: "HrlRolloutStorage"
  updater_name: "HrlPPO"
  distrib_updater_name: "HrlDDPPO"
  test_episode_count: -1
  num_environments: 1
  num_updates: -1
  total_num_steps: 1.0e8
  log_interval: 10
  num_checkpoints: 20
  force_torch_single_threaded: True
  eval_keys_to_include_in_name: ['reward', 'force', 'composite_success']
  load_resume_state_config: False

  eval:
    use_ckpt_config: False
    should_load_ckpt: False
    video_option: ["disk"]

  rl:
    policy:
        name: "HierarchicalPolicy"
        obs_transforms:
          add_virtual_keys:
            virtual_keys:
              "goal_to_agent_gps_compass": 2
        hierarchical_policy:
          high_level_policy:
            name: "FixedHighLevelPolicy"
            add_arm_rest: True
          defined_skills:
            open_cab:
              skill_name: "NoopSkillPolicy"
              max_skill_steps: 1
              apply_postconds: True

            open_fridge:
              skill_name: "NoopSkillPolicy"
              max_skill_steps: 1
              apply_postconds: True

            close_cab:
              skill_name: "NoopSkillPolicy"
              obs_skill_inputs: ["obj_start_sensor"]
              max_skill_steps: 1

            close_fridge:
              skill_name: "NoopSkillPolicy"
              obs_skill_inputs: ["obj_start_sensor"]
              max_skill_steps: 1
              apply_postconds: True

            pick:
              skill_name: "NoopSkillPolicy"
              obs_skill_inputs: ["obj_start_sensor"]
              max_skill_steps: 1
              apply_postconds: True

            place:
              skill_name: "NoopSkillPolicy"
              obs_skill_inputs: ["obj_goal_sensor"]
              max_skill_steps: 1
              apply_postconds: True

            nav_to_obj:
              skill_name: "OracleNavPolicy"
              obs_skill_inputs: ["obj_start_sensor", "abs_obj_start_sensor", "obj_goal_sensor", "abs_obj_goal_sensor"]
              max_skill_steps: 300

            wait_skill:
              skill_name: "WaitSkillPolicy"
              max_skill_steps: -1
              force_end_on_timeout: False

            reset_arm_skill:
              skill_name: "ResetArmSkill"
              max_skill_steps: 50
              reset_joint_state: [-4.5003259e-01, -1.0799699e00, 9.9526465e-02, 9.3869519e-01, -7.8854430e-04, 1.5702540e00, 4.6168058e-03]
              force_end_on_timeout: False

          use_skills:
            open_cab: "open_cab"
            open_fridge: "open_fridge"
            close_cab: "close_cab"
            close_fridge: "close_fridge"
            pick: "pick"
            place: "place"
            nav: "nav_to_obj"
            nav_to_receptacle: "nav_to_obj"
            wait: "wait_skill"
            reset_arm: "reset_arm_skill"

    ddppo:
      sync_frac: 0.6
      # The PyTorch distributed backend to use
      distrib_backend: NCCL
      # Visual encoder backbone
      pretrained_weights: data/ddppo-models/gibson-2plus-resnet50.pth
      # Initialize with pretrained weights
      pretrained: False
      # Initialize just the visual encoder backbone with pretrained weights
      pretrained_encoder: False
      # Whether the visual encoder backbone will be trained.
      train_encoder: True
      # Whether to reset the critic linear layer
      reset_critic: False

      # Model parameters
      backbone: resnet18
      rnn_type: LSTM
      num_recurrent_layers: 2
