{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook file demonstrating how to extract camera parameters in the scene and how these camera parameters relate to the given views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import quaternion\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import habitat\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "os.chdir('./..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment for testing\n",
    "config = habitat.get_config(config_paths=\"./configs/tasks/pointnav_rgbd.yaml\")\n",
    "\n",
    "# Can also do directly in the config file\n",
    "config.defrost()\n",
    "config.SIMULATOR.DEPTH_SENSOR.NORMALIZE_DEPTH = False\n",
    "config.freeze()\n",
    "\n",
    "# Intrinsic parameters, assuming width matches height. Requires a simple refactor otherwise\n",
    "W = config.SIMULATOR.DEPTH_SENSOR.WIDTH\n",
    "H = config.SIMULATOR.DEPTH_SENSOR.HEIGHT\n",
    "\n",
    "assert(W == H)\n",
    "hfov = float(config.SIMULATOR.DEPTH_SENSOR.HFOV) * np.pi / 180.\n",
    "\n",
    "\n",
    "env = habitat.Env(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly permute the rotation\n",
    "\n",
    "def uniform_quat(original_angle):\n",
    "    original_euler = quaternion.as_euler_angles(original_angle)\n",
    "    euler_angles = np.array([(np.random.rand() - 0.5) * np.pi / 9. + original_euler[0],\n",
    "                             (np.random.rand() - 0.5) * np.pi / 9. + original_euler[1],\n",
    "                             (np.random.rand() - 0.5) * np.pi / 9. + original_euler[2]])\n",
    "    quaternions = quaternion.from_euler_angles(euler_angles)\n",
    "    \n",
    "    \n",
    "    return quaternions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two random, overlapping views\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "depths = []\n",
    "rgbs = []\n",
    "cameras = []\n",
    "\n",
    "initial_state = env._sim.get_agent_state(0)\n",
    "init_translation = initial_state.position\n",
    "init_rotation = initial_state.rotation\n",
    "\n",
    "\n",
    "for i in range(0, 2):\n",
    "    rotation = uniform_quat(init_rotation)\n",
    "    translation = init_translation + np.random.rand(3,) * 0.5 - 0.25\n",
    "\n",
    "    env._sim.set_agent_state(agent_id=0, position=translation, \n",
    "                         rotation=rotation)\n",
    "\n",
    "    rgb = env.render(mode='rgb')\n",
    "    depth = env.render(mode='depth')[:,:,0]\n",
    "\n",
    "    depths += [depth]\n",
    "    rgbs += [rgb]\n",
    "    cameras += [env._sim.get_agent_state()]\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrinsic parameters, K\n",
    "\n",
    "K = np.array([\n",
    "    [1 / np.tan(hfov / 2.), 0., 0., 0.],\n",
    "    [0., - 1 / np.tan(hfov / 2.), 0., 0.],\n",
    "    [0., 0.,  -1, 0],\n",
    "    [0., 0., 0, 1]])\n",
    "\n",
    "# Now get an approximation for the true world coordinates -- see if they make sense\n",
    "xs, ys = np.meshgrid(np.linspace(-1,1,W), np.linspace(-1,1,W)) \n",
    "depth = depths[0].reshape(1,W,W) \n",
    "xs = xs.reshape(1,W,W)\n",
    "ys = ys.reshape(1,W,W)\n",
    "\n",
    "# Unproject\n",
    "xys = np.vstack((xs * depth , ys * depth, depth, np.ones(depth.shape)))\n",
    "xys = xys.reshape(4, -1)\n",
    "\n",
    "xyc = np.matmul(np.linalg.inv(K), xys)\n",
    "\n",
    "# Now load in the cameras, are in the format camera --> world\n",
    "# Camera 1:\n",
    "quaternion_0 = cameras[0].sensor_states['depth'].rotation\n",
    "translation_0 = cameras[0].sensor_states['depth'].position\n",
    "rotation_0 = - quaternion.as_rotation_matrix(quaternion_0)\n",
    "wrld_rt_0 = np.eye(4)\n",
    "wrld_rt_0[0:3,0:3] = rotation_0\n",
    "wrld_rt_0[0:3,3] = - translation_0\n",
    "\n",
    "# Camera 2:\n",
    "translation_1 = cameras[1].sensor_states['depth'].position\n",
    "quaternion_1 = cameras[1].sensor_states['depth'].rotation\n",
    "rotation_1 = - quaternion.as_rotation_matrix(quaternion_1)\n",
    "wrld_rt_1 = np.eye(4)\n",
    "wrld_rt_1[0:3,0:3] =  rotation_1\n",
    "wrld_rt_1[0:3,3] = - translation_1\n",
    "\n",
    "# Invert to get world --> camera\n",
    "rt_1 = np.linalg.inv(wrld_rt_1)\n",
    "\n",
    "# Transformation matrix between views\n",
    "RT = np.matmul(rt_1, wrld_rt_0)\n",
    "\n",
    "# Finally transform actual points\n",
    "xy_c2 = np.matmul(RT, xyc)\n",
    "xy_newimg = np.matmul(K, xy_c2)\n",
    "\n",
    "xys_newimg = xy_newimg[0:2,:] / xy_newimg[2:3,:]\n",
    "xys_newimg[0].reshape(W,W)\n",
    "print('Transformed points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And visualise this new transformation\n",
    "\n",
    "# Create sampler\n",
    "sampler = torch.Tensor(xys_newimg).view(2, W, W).permute(1,2,0).unsqueeze(0)\n",
    "\n",
    "# Create generated image\n",
    "img1_tensor = ToTensor()(rgbs[0]).unsqueeze(0)\n",
    "img2_tensor = ToTensor()(rgbs[1]).unsqueeze(0)\n",
    "img2_warped = F.grid_sample(img2_tensor, sampler)\n",
    "\n",
    "# Visualise\n",
    "plt.figure(figsize=(10,10))\n",
    "ax1 = plt.subplot(221)\n",
    "ax1.imshow(img1_tensor.squeeze().permute(1,2,0))\n",
    "ax1 = plt.subplot(222)\n",
    "ax1.imshow(img2_tensor.squeeze().permute(1,2,0))\n",
    "ax1 = plt.subplot(223)\n",
    "ax1.imshow(np.abs(img2_warped.squeeze().permute(1,2,0) - img1_tensor.squeeze().permute(1,2,0)))\n",
    "\n",
    "ax1 = plt.subplot(224)\n",
    "plt.imshow(img2_warped.squeeze().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
